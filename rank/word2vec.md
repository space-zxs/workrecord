## 关于word2vec和embedding
***

***one_hot 和 embedding***

> https://zhuanlan.zhihu.com/p/146117421

独热编码，是一位有效编码，任何时候都只有一位有效位。用于将分类变量转变为二进制向量表示，以预测最终的分类类别。

embedding 

简单来说，embedding就是用一个低维的向量表示一个物体，可以是一个词，或是一个商品，或是一个电影等等。这个embedding向量的性质是能使距离相近的向量对应的物体有相近的含义，比如 Embedding(复仇者联盟)和Embedding(钢铁侠)之间的距离就会很接近，但 Embedding(复仇者联盟)和Embedding(乱世佳人)的距离就会远一些。

言归正传，Embedding能够用低维向量对物体进行编码还能保留其含义的特点非常适合深度学习。在传统机器学习模型构建过程中，我们经常使用one hot encoding对离散特征，但由于one hot encoding的维度等于物体的总数，比如阿里的商品one hot encoding的维度就至少是千万量级的。这样的编码方式对于商品来说是极端稀疏的，甚至用multi hot encoding对用户浏览历史的编码也会是一个非常稀疏的向量。而深度学习的特点以及工程方面的原因使其不利于稀疏特征向量的处理。因此如果能把物体编码为一个低维稠密向量再喂给DNN（深度神经网络），自然是一个高效的基本操作。

embedding 方法的空前流行还是得益于word2vec的流行

**关于embedding和word2vec的几个问题总结**

为什么说深度学习的特点不适合处理特征过于稀疏的样本？

一方面，如果我们深入到神经网络的梯度下降学习过程就会发现，特征过于稀疏会导致整个网络的收敛非常慢，因为每一个样本的学习只有极少数的权重会得到更新，这在样本数量有限的情况下会导致模型不收敛。

另一个方面，One-hot 类稀疏特征的维度往往非常地大，可能会达到千万甚至亿的级别，如果直接连接进入深度学习网络，那整个模型的参数数量会非常庞大，这对于一般公司的算力开销都是吃不消的。

我们能把输出矩阵中的权重向量当作词向量吗？

两者都可以用，甚至可以拿输入矩阵转置作为输出矩阵，也没必要存储两个矩阵，不管CBOW还是skip-gram都能保证相似语境下的相关性

为什么在计算word similarity的时候，我们要用cosine distance，我们能够用其他距离吗？

w2v本身就是内积，用cos很合适，还归一化了

在word2vec的目标函数中，两个词Wi,Wj的词向量Vi,Vj其实分别来自输入权重矩阵和输出权重矩阵，那么在实际使用时，我们需要分别存储输入矩阵和输出矩阵吗？还是直接用输入矩阵当作word2vec计算similarity就好了？

不需要，直接用存储输入矩阵使用

隐层的激活函数是什么？是sigmoid吗？

是sigmoid

**softmax和sigmoid函数的不同**

sigmoid 函数用于多标签问题，选取多个标签作为正确答案，它是将任意实数值归一化映射到[0-1]之间，并不是不同概率之间的相互关联，且由于远离0的部分梯度较小，容易出现梯度消失现象 

 Softmax 函数用于多分类问题，即从多个分类中选取一个正确答案。 Softmax 综合了所有输出值的归一化，因此得到的是不同概率之间的相互关联 。它将任意实数值的 K 维向量 z“压缩”（映射）到 (0, 1) 范围内加起来为 1 的实数值的 K 维向量 σ(z)。
 
 ## word2vec
 
 **向量点乘的几何意义**

点乘的几何意义是：是一条边向另一条边的投影乘以另一条边的长度。

虽然独热向量很容易构建，但它们通常不是一个好的选择。一个主要原因是独热向量不能准确表达不同词之间的相似度。因为两个不同词的独热向量的乘积是零。

word2vec的提出解决了上述问题。它将每个词映射到一个固定长度的向量，这些向量能更好地表达不同词之间的相似性和类比关系。word2vec工具包含两个模型，即跳元模型（skip-gram）和连续词袋（CBOW）。对于在语义上有意义的表示，它们的训练依赖于条件概率，条件概率可以被看作是使用语料库中一些词来预测另一些单词。由于是不带标签的数据，因此跳元模型和连续词袋都是自监督模型

**跳元模型（Skip-Gram）**

条件概率P(A|B) 表示在B成立的条件下A成立的概率

跳元模型假设一个词可以用来在文本序列中生成其周围的单词。以文本序列“the”、“man”、“loves”、“his”、“son”为例。假设中心词选择“loves”，并将上下文窗口设置为2，给定中心词“loves”，跳元模型考虑生成上下文词“the”、“man”、“him”、“son”的条件概率：P("the","man","his","son"|"loves")

假设上下文词是在给定中心词的情况下独立生成的（即条件独立性）。在这种情况下，上述条件概率可以重写为：P("the"|"loves")*P("man"|"loves")*P("his"|"loves")*P("son"|"loves")

![image](https://user-images.githubusercontent.com/77714764/195579533-2c048caa-e9a9-42c5-9c09-03dfca70f58b.png)

在跳元模型中，每个词都有两个d维向量表示，用于计算条件概率。

## 连续词袋（CBOW）模型

连续词袋（CBOW）模型类似于跳元模型。与跳元模型的主要区别在于，连续词袋模型假设中心词是基于其在文本序列中的周围上下文词生成的。例如，在文本序列“the”、“man”、“loves”、“his”、“son”中，在“loves”为中心词且上下文窗口为2的情况下，连续词袋模型考虑基于上下文词“the”、“man”、“him”、“son” 生成中心词“loves”的条件概率，即：P("loves"|"the","man","his","son")

![image](https://user-images.githubusercontent.com/77714764/195580190-ab1659f2-f51f-481a-be51-6addd54fc3c3.png)

由于连续词袋模型中存在多个上下文词，因此在计算条件概率时对这些上下文词向量进行平均。

**小结**

- 词向量是用于表示单词意义的向量，也可以看作是词的特征向量。将词映射到实向量的技术称为词嵌入。

- word2vec工具包含跳元模型和连续词袋模型。

- 跳元模型假设一个单词可用于在文本序列中，生成其周围的单词；而连续词袋模型假设基于上下文词来生成中心单词。
