# CTR 预估
***

**CTR的预估常用在推荐系统的排序中，给召回中的数据一个预测值，更高预测值的数据被推荐给用户的可能性越高**

**上采样/下采样 ,过采样/欠采样,解决样本不均衡问题**

过抽样（样本数量少时使用）

过抽样（也叫上采样、over-sampling）方法通过增加分类中少数类样本的数量来实现样本均衡，最直接的方法是简单复制少数类样本形成多条记录，这种方法的缺点是如果样本特征少而可能导致过拟合的问题；经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本，例如SMOTE算法。

欠抽样（样本数量多时使用）

欠抽样（也叫下采样、under-sampling）方法通过减少分类中多数类样本的样本数量来实现样本均衡，最直接的方法是随机地去掉一些多数类样本来减小多数类的规模，缺点是会丢失多数类样本中的一些重要信息。

**正/负采样**

通过正负样本的惩罚权重解决样本不均衡

**协同过滤**

协同过滤（Collaborative Filtering，CF）是推荐算法的鼻祖，至今各个互联网公司中，CF都扮演着不可或缺的角色。协同过滤是根据大家的反馈、评论和意见一起对海量的信息进行过滤，从中筛选出目标用户可能感兴趣的信息的推荐过程。大致过程是，将用户与商品放入一个共现矩阵中，矩阵中的值为某用户对某件商品的点击、评价或者购买行为的度量。我们可以将用户感兴趣的所有商品向量化，记为代表该用户的向量，进而可以计算用户间的相似度。于是，可以将与某待推荐用户相似的用户所感兴趣的商品推荐给该用户。类似的，可以将对商品感兴趣的所有用户向量化，记为代表该商品的向量，进而计算物品之间的相似度。在实际的计算过程中，还应该对爆品、高销品与其他商品的相似度进行一定程度上的衰减。相似度的计算也应该进行归一化，排除数量级的影响。

协同过滤的优点是没有显式的学习过程、可解释性强、简单、速度快。其缺点也很明显：协同过滤只考虑了用户和物品的id信息，而无法将用户的属性、物品的属性、上下文考虑在内，无法挖掘用户和物品之间的隐含关系。对于没有购买或者消费的新用户，协同过滤不知如何推荐，泛化性能差，推荐头部效应比较明显。针对这些问题，MF（Matrix Factorization，矩阵分解）模型被提出来。它的核心思想是通过两个低维小矩阵（一个代表用户embedding矩阵，一个代表物品embedding矩阵）的乘积计算，来模拟真实用户点击或评分产生的大的协同信息稀疏矩阵，本质上是编码了用户和物品协同信息的降维模型。当训练完成，每个用户和物品得到对应的低维embedding表达后，如果要预测某个 对  的评分的时候，只要它们做个内积计算$$，这个得分就是预测得分。

本质上，MF模型是FM模型的特例，MF可以被认为是只有User ID 和Item ID这两个特征Fields的FM模型，MF将这两类特征通过矩阵分解，来达到将这两类特征embedding化表达的目的。而FM则可以看作是MF模型的进一步拓展，除了User ID和Item ID这两类特征外，很多其它类型的特征，都可以进一步融入FM模型里，它将所有这些特征转化为embedding低维向量表达，并计算任意两个特征embedding的内积，就是特征组合的权重，如果FM只使用User ID 和Item ID，你套到FM公式里，看看它的预测过程和MF的预测过程一样吗？

在具体的实践过程中，FM模型和MF模型相比，前者继承了后者特征embedding化的特点，同时引入了更多的Side information作为特征，将更多的特征及Side information embedding化融入FM模型中。所以表现的也更加的灵活，能够适应更多的场景。在推荐排序阶段，绝大多数只使用ID信息的模型是不实用的，没有引入Side Information（也就是除了User ID／Item ID外的很多其它可用特征的模型）是不具备实战价值的。原因很简单，大多数真实应用场景中，User/Item有很多信息可用，而协同数据只是其中的一种，引入更多特征明显对于更精准地进行个性化推荐是非常有帮助的。而如果模型不支持更多特征的便捷引入，明显受限严重，很难真正实用，这也是为何矩阵分解类的方法很少看到在排序阶段使用，通常是作为一路召回形式存在的原因。

### LR 

LR模型是广义线性模型，从其函数形式来看，LR模型可以看做是一个没有隐层的神经网络模型（感知机模型）。

![image](https://user-images.githubusercontent.com/77714764/195815059-c988e7cc-d6b6-4b84-b970-f4bae48248de.png)

LR模型一直是CTR预估问题的benchmark模型，由于其简单、易于并行化实现、可解释性强等优点而被广泛使用。然而由于线性模型本身的局限，不能处理特征和目标之间的非线性关系

### LR + GBDT 

既然特征工程很难，那能否自动完成呢？模型级联提供了一种思路，典型的例子就是Facebook 2014年的论文中介绍的通过GBDT（Gradient Boost Decision Tree）模型解决LR模型的特征组合问题。思路很简单，特征工程分为两部分，一部分特征用于训练一个GBDT模型，把GBDT模型每颗树的叶子节点编号作为新的特征，加入到原始特征集中，再用LR模型训练最终的模型。

![image](https://user-images.githubusercontent.com/77714764/195815413-f9d5bc78-c90d-48d8-a617-04297394d293.png)

GBDT模型能够学习高阶非线性特征组合，对应树的一条路径（用叶子节点来表示）。通常把一些连续值特征、值空间不大的categorical特征都丢给GBDT模型；空间很大的ID特征（比如商品ID）留在LR模型中训练，既能做高阶特征组合又能利用线性模型易于处理大规模稀疏数据的优势。

### FM FFM

FM和基于树的模型（e.g. GBDT）都能够自动学习特征交叉组合。基于树的模型适合连续中低度稀疏数据，容易学到高阶组合。但是树模型却不适合学习高度稀疏数据的特征组合，一方面高度稀疏数据的特征维度一般很高，这时基于树的模型学习效率很低，甚至不可行；另一方面树模型也不能学习到训练数据中很少或没有出现的特征组合。相反，FM模型因为通过隐向量的内积来提取特征组合，对于训练数据中很少或没有出现的特征组合也能够学习到。例如，特征  和特征  在训练数据中从来没有成对出现过，但特征  经常和特征  成对出现，特征  也经常和特征  成对出现，因而在FM模型中特征  和特征  也会有一定的相关性。毕竟所有包含特征  的训练样本都会导致模型更新特征  的隐向量 ，同理，所有包含特征  的样本也会导致模型更新隐向量  ，这样  就不太可能为0。

在推荐系统中，常用矩阵分解（MF）的方法把User-Item评分矩阵分解为两个低秩矩阵的乘积，这两个低秩矩阵分别为User和Item的隐向量集合。通过User和Item隐向量的点积来预测用户对未见过的物品的兴趣。矩阵分解也是生成embedding表示的一种方法，示例图如下：

![image](https://user-images.githubusercontent.com/77714764/195816669-14888839-455e-4723-a505-84daf7f51eaa.png)

MF方法可以看作是FM模型的一种特例，即MF可以看作特征只有userId和itemId的FM模型。FM的优势是能够将更多的特征融入到这个框架中，并且可以同时使用一阶和二阶特征；而MF只能使用两个实体的二阶特征。
![image](https://user-images.githubusercontent.com/77714764/195816920-9a8776d0-9ad8-42be-b090-86570d4a92a2.png)

FFM（Field-aware Factorization Machine）模型是对FM模型的扩展，通过引入field的概念，FFM把相同性质的特征归于同一个field。例如，“Day=26/11/15”、 “Day=1/7/14”、 “Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，商品的末级品类编码也可以放到同一个field中。

### 混合逻辑回归（MLR）

MLR算法是alibaba在2012年提出并使用的广告点击率预估模型，2017年发表出来。MLR模型是对线性LR模型的推广，它利用分片线性方式对数据进行拟合。基本思路是采用分而治之的策略：如果分类空间本身是非线性的，则按照合适的方式把空间分为多个区域，每个区域里面可以用线性的方式进行拟合，最后MLR的输出就变为了多个子区域预测值的加权平均。如下图(C)所示，就是使用4个分片的MLR模型学到的结果.

![image](https://user-images.githubusercontent.com/77714764/195817822-20dd9e75-fbee-4e46-99a1-ce55eddba6c1.png)

MLR模型在大规模稀疏数据上探索和实现了非线性拟合能力，在分片数足够多时，有较强的非线性能力；同时模型复杂度可控，有较好泛化能力；同时保留了LR模型的自动特征选择能力。

MLR模型的思路非常简单，难点和挑战在于MLR模型的目标函数是非凸非光滑的，使得传统的梯度下降算法并不适用。相关的细节内容查询论文：Gai et al, “Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction” 。

### WDL

像LR这样的wide模型学习特征与目标之间的直接相关关系，偏重记忆（memorization），如在推荐系统中，wide模型产生的推荐是与用户历史行为的物品直接相关的物品。这样的模型缺乏刻画特征之间的关系的能力，比如模型无法感知到“土豆”和“马铃薯”是相同的实体，在训练样本中没有出现的特征组合自然就无法使用，因此可能模型学习到某种类型的用户喜欢“土豆”，但却会判定该类型的用户不喜欢“马铃薯”。

WDL是Google在2016年的paper中提出的模型，其巧妙地将传统的特征工程与深度模型进行了强强联合。模型结构如下:

![image](https://user-images.githubusercontent.com/77714764/195822231-1a4ccfa9-5bda-411c-a0c3-0ff0b0a4898e.png)

WDL分为wide和deep两部分联合训练，单看wide部分与LR模型并没有什么区别；deep部分则是先对不同的ID类型特征做embedding，在embedding层接一个全连接的MLP（多层感知机），用于学习特征之间的高阶交叉组合关系。由于Embedding机制的引入，WDL相对于单纯的wide模型有更强的泛化能力.

### FNN(Factorization-machine supported Neural Network)

除了神经网络模型，FM模型也可以用来学习到特征的隐向量（embedding表示），因此一个自然的想法就是先用FM模型学习到特征的embedding表示，再用学到的embedding向量代替原始特征作为最终模型的特征。这个思路类似于LR+GBDT，整个学习过程分为两个阶段：第一个阶段先用一个模型做特征工程；第二个阶段用第一个阶段学习到新特征训练最终的模型。

FNN模型就是用FM模型学习到的embedding向量初始化MLP，再由MLP完成最终学习，其模型结构如下：
![image](https://user-images.githubusercontent.com/77714764/195824160-3b682813-2651-4a37-a315-aea263b22c00.png)

### PNN（Product-based Neural Networks）

MLP中的节点add操作可能不能有效探索到不同类别数据之间的交互关系，虽然MLP理论上可以以任意精度逼近任意函数，但越泛化的表达，拟合到具体数据的特定模式越不容易。PNN主要是在深度学习网络中增加了一个inner/outer product layer，用来建模特征之间的关系。
![image](https://user-images.githubusercontent.com/77714764/195827375-e6d8e2b8-93c0-4b33-95ee-0a2740d2be1a.png)

### DeepFm

深度神经网络对于学习复杂的特征关系非常有潜力。目前也有很多基于CNN与RNN的用于CTR预估的模型。但是基于CNN的模型比较偏向于相邻的特征组合关系提取，基于RNN的模型更适合有序列依赖的点击数据。

FNN模型首先预训练FM，再将训练好的FM应用到DNN中。PNN网络的embedding层与全连接层之间加了一层Product Layer来完成特征组合。PNN和FNN与其他已有的深度学习模型类似，都很难有效地提取出低阶特征组合。WDL模型混合了宽度模型与深度模型，但是宽度模型的输入依旧依赖于特征工程。

上述模型要不然偏向于低阶特征或者高阶特征的提取，要不然依赖于特征工程。而DeepFM模型可以以端对端的方式来学习不同阶的组合特征关系，并且不需要其他特征工程。

DeepFM的结构中包含了因子分解机部分以及深度神经网络部分，分别负责低阶特征的提取和高阶特征的提取。其结构如下：

![image](https://user-images.githubusercontent.com/77714764/195837461-4ff13c84-98d2-4f6c-a963-480fd872bf88.png)

上图中红色箭头所表示的链接权重恒定为1（weight-1 connection），在训练过程中不更新，可以认为是把节点的值直接拷贝到后一层，再参与后一层节点的运算操作。

与Wide&Deep Model不同，DeepFM共享相同的输入与embedding向量。在Wide&Deep Model中，因为在Wide部分包含了人工设计的成对特征组，所以输入向量的长度也会显著增加，这也增加了复杂性。

FM部分的详细结构如下：
![image](https://user-images.githubusercontent.com/77714764/195838782-b3ca8a22-2c57-41a3-8014-4308b44e62ce.png)

深度部分详细如下：
![image](https://user-images.githubusercontent.com/77714764/195838837-02651400-47fe-4409-95bb-c4e70a170482.png)

深度部分是一个前馈神经网络。与图像或者语音这类输入不同，图像语音的输入一般是连续而且密集的，然而用于CTR的输入一般是及其稀疏的。因此需要设计特定的网络结构，具体实现为，在第一层隐含层之前，引入一个嵌入层来完成将输入向量压缩到低维稠密向量。

### DIN

DIN是阿里17年的论文中提出的深度学习模型，该模型基于对用户历史行为数据的两个观察：1、多样性，一个用户可能对多种品类的东西感兴趣；2、部分对应，只有一部分的历史数据对目前的点击预测有帮助，比如系统向用户推荐泳镜时会与用户点击过的泳衣产生关联，但是跟用户买的书就关系不大。于是，DIN设计了一个attention结构，对用户的历史数据和待估算的广告之间部分匹配，从而得到一个权重值，用来进行embedding间的加权求和。
![image](https://user-images.githubusercontent.com/77714764/195838986-8f55d652-2fd9-4aca-b300-60fb18e86705.png

DIN模型的输入分为2个部分：用户特征和广告(商品)特征。用户特征由用户历史行为的不同实体ID序列组成。在对用户的表示计算上引入了attention network (也即图中的Activation Unit) 。DIN把用户特征、用户历史行为特征进行embedding操作，视为对用户兴趣的表示，之后通过attention network，对每个兴趣表示赋予不同的权值。这个权值是由用户的兴趣和待估算的广告进行匹配计算得到的，如此模型结构符合了之前的两个观察：用户兴趣的多峰分布以及部分对应。
