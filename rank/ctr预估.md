# CTR 预估
***

**CTR的预估常用在推荐系统的排序中，给召回中的数据一个预测值，更高预测值的数据被推荐给用户的可能性越高**

***one_hot 和 embedding***

> https://zhuanlan.zhihu.com/p/146117421

独热编码，是一位有效编码，任何时候都只有一位有效位。用于将分类变量转变为二进制向量表示，以预测最终的分类类别。

embedding 

简单来说，embedding就是用一个低维的向量表示一个物体，可以是一个词，或是一个商品，或是一个电影等等。这个embedding向量的性质是能使距离相近的向量对应的物体有相近的含义，比如 Embedding(复仇者联盟)和Embedding(钢铁侠)之间的距离就会很接近，但 Embedding(复仇者联盟)和Embedding(乱世佳人)的距离就会远一些。

言归正传，Embedding能够用低维向量对物体进行编码还能保留其含义的特点非常适合深度学习。在传统机器学习模型构建过程中，我们经常使用one hot encoding对离散特征，但由于one hot encoding的维度等于物体的总数，比如阿里的商品one hot encoding的维度就至少是千万量级的。这样的编码方式对于商品来说是极端稀疏的，甚至用multi hot encoding对用户浏览历史的编码也会是一个非常稀疏的向量。而深度学习的特点以及工程方面的原因使其不利于稀疏特征向量的处理。因此如果能把物体编码为一个低维稠密向量再喂给DNN（深度神经网络），自然是一个高效的基本操作。

embedding 方法的空前流行还是得益于word2vec的流行

**关于embedding和word2vec的几个问题总结**

为什么说深度学习的特点不适合处理特征过于稀疏的样本？

一方面，如果我们深入到神经网络的梯度下降学习过程就会发现，特征过于稀疏会导致整个网络的收敛非常慢，因为每一个样本的学习只有极少数的权重会得到更新，这在样本数量有限的情况下会导致模型不收敛。

另一个方面，One-hot 类稀疏特征的维度往往非常地大，可能会达到千万甚至亿的级别，如果直接连接进入深度学习网络，那整个模型的参数数量会非常庞大，这对于一般公司的算力开销都是吃不消的。

我们能把输出矩阵中的权重向量当作词向量吗？

两者都可以用，甚至可以拿输入矩阵转置作为输出矩阵，也没必要存储两个矩阵，不管CBOW还是skip-gram都能保证相似语境下的相关性

为什么在计算word similarity的时候，我们要用cosine distance，我们能够用其他距离吗？

w2v本身就是内积，用cos很合适，还归一化了

在word2vec的目标函数中，两个词Wi,Wj的词向量Vi,Vj其实分别来自输入权重矩阵和输出权重矩阵，那么在实际使用时，我们需要分别存储输入矩阵和输出矩阵吗？还是直接用输入矩阵当作word2vec计算similarity就好了？

不需要，直接用存储输入矩阵使用

隐层的激活函数是什么？是sigmoid吗？

是sigmoid

**softmax和sigmoid函数的不同**

sigmoid 函数用于多标签问题，选取多个标签作为正确答案，它是将任意实数值归一化映射到[0-1]之间，并不是不同概率之间的相互关联，且由于远离0的部分梯度较小，容易出现梯度消失现象 

 Softmax 函数用于多分类问题，即从多个分类中选取一个正确答案。 Softmax 综合了所有输出值的归一化，因此得到的是不同概率之间的相互关联 。它将任意实数值的 K 维向量 z“压缩”（映射）到 (0, 1) 范围内加起来为 1 的实数值的 K 维向量 σ(z)。
 
 
