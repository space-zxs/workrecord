## 资源去重

> https://www.cnblogs.com/kuangdaoyizhimei/p/15624703.html#_caption0

***对资源进行去重，比如论文去重，也就是计算论文之间的相似度。***


**TF **

TF(term frequency),就是分词出现的频率：该分词在该文档中出现的频率，算法是：（该分词在该文档出现的次数）/(该文档分词的总数)，这个值越大表示这个词越重要，即权重就越大。例如：一篇文档分词后，总共有500个分词，而分词”Hello”出现的次数是20次，则TF值是： tf =20/500=2/50=0.04

例如：一篇文档分词后，总共有500个分词，而分词”Hello”出现的次数是20次，则TF值是： tf =20/500=2/50=0.04

**IDF**

IDF(inverse-document-frequency)逆向文件频率,一个文档库中，一个分词出现在的文档数越少越能和其它文档区别开来。算法是： log((总文档数/出现该分词的文档数)+0.01) ；（注加上0.01是为了防止log计算返回值为0）。

例如：一个文档库中总共有50篇文档，2篇文档中出现过“Hello”分词，则idf是：

Idf = log(50/2 + 0.01) = log(25.01)=1.39811369

**TF-IDF**

TF-IDF（termfrequency–inverse document frequency）是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随著它在文件中出现的次数成正比增加，但同时会随著它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜寻引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了TF-IDF以外，因特网上的搜寻引擎还会使用基于连结分析的评级方法，以确定文件在搜寻结果中出现的顺序。

在文本挖掘中，要对文本库分词，而分词后需要对个每个分词计算它的权重，而这个权重可以使用TF-IDF计算。

TF-IDF结合计算就是 tf*idf,比如上面的“Hello”分词例子中：

TF-IDF = tf* idf = (20/500)* log(50/2 + 0.01)= 0.04*1.39811369=0.0559245476

**准确率和召回率**

召回率(Recall Rate,也叫查全率)是检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率；

准确率是检索出的相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率。

![image](https://user-images.githubusercontent.com/77714764/196126399-223ef38d-4ef6-4ede-b2f8-1263d8f7ff60.png)

**编辑距离**
编辑距离（Minimum Edit Distance，MED），由俄罗斯科学家 Vladimir Levenshtein 在1965年提出，也因此而得名 Levenshtein Distance。

在信息论、语言学和计算机科学领域，Levenshtein Distance 是用来度量两个序列相似程度的指标。通俗地来讲，编辑距离指的是在两个单词<w_1,w_2>之间，由其中一个单词w_1转换为另一个单词w_2所需要的最少单字符编辑操作次数。

在这里定义的单字符编辑操作有且仅有三种：

插入（Insertion）
删除（Deletion）
替换（Substitution）
譬如，"kitten" 和 "sitting" 这两个单词，由 "kitten" 转换为 "sitting" 需要的最少单字符编辑操作有：

1.kitten → sitten (substitution of "s" for "k")
2.sitten → sittin (substitution of "i" for "e")
3.sittin → sitting (insertion of "g" at the end)

因此，"kitten" 和 "sitting" 这两个单词之间的编辑距离为 3 。

**汉明距离（海明距离）**
简单的说，Hamming Distance，又称汉明距离，在信息论中，两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数。也就是说，它就是将一个字符串变换成另外一个字符串所需要替换的字符个数。例如：1011101 与 1001001 之间的汉明距离是 2。至于我们常说的字符串编辑距离则是一般形式的汉明距离。

海明距离的求法：异或时，只有在两个比较的位不同时其结果是1 ，否则结果为0，两个二进制“异或”后得到1的个数即为海明距离的大小。

**鸽巢原理**
桌上有十个苹果，要把这十个苹果放到九个抽屉里，无论怎样放，我们会发现至少会有一个抽屉里面放不少于两个苹果。这一现象就是我们所说的“抽屉原理”。 抽屉原理的一般含义为：“如果每个抽屉代表一个集合，每一个苹果就可以代表一个元素，假如有n+1个元素放到n个集合中去，其中必定有一个集合里至少有两个元素。”

抽屉原理有时也被称为鸽巢原理，它是组合数学中一个重要的原理 。

**倒排索引**
在搜索引擎中每个文件都对应一个文件ID，文件内容被表示为一系列关键词的集合（实际上在搜索引擎索引库中，关键词也已经转换为关键词ID）。例如“文档1”经过分词，提取了20个关键词，每个关键词都会记录它在文档中的出现次数和出现位置。

得到正向索引的结构如下：

​ “文档1”的ID > 单词1：出现次数，出现位置列表；单词2：出现次数，出现位置列表；…………。

​ “文档2”的ID > 此文档出现的关键词列表。

![image](https://user-images.githubusercontent.com/77714764/196126726-057d1b2f-a4ef-46d1-bca8-03f80410c043.png)


**正向索引**

根据正向索引，查找的时候是挨个文档查找对应的关键词，在数据量大的情况下效率会很低，典型的是mysql的like查询方式。

倒排索引则反之，是根据关键词查询文档的技术。

倒排索引的结构如下：

![image](https://user-images.githubusercontent.com/77714764/196126792-e2bb74fd-0b0d-4bdf-a048-bd5e9606ed45.png)

倒排索引

倒排索引的典型应用是ElasticSearch。

### simhash算法及原理简介

简单的说，SimHash算法主要的工作就是将文本进行降维，生成一个SimHash值，也就是论文中所提及的“指纹”，通过对不同文本的SimHash值进而比较海明距离，从而判断两个文本的相似度。

主要步骤是： 分词、hash、加权、合并、降维

海量数据下的汉明距离的计算（使用鸽巢原理优化），提升查找效率

### Minhash算法及原理介绍
给出N个集合，找到相似的集合对，如何实现呢？直观的方法是比较任意两个集合。那么可以十分精确的找到每一对相似的集合，但是时间复杂度是O(n2)。当N比较小时，比如K级，此算法可以在接受的时间范围内完成，但是如果N变大时，比B级，甚至P级，那么需要的时间是不能够被接受的。比如N= 1B = 1,000,000,000。一台计算机每秒可以比较1,000,000,000对集合是否相等。那么大概需要15年的时间才能找到所有相似的集合！

上面的算法虽然效率很低，但是结果会很精确，因为检查了每一对集合。假如，N个集合中只有少数几对集合相似，绝大多数集合都不等呢？那么根据上述算法，绝大多数检测的结果是两个结合不相似，可以说这些检测“浪费了计算时间”。所以，如果能找到一种算法，将大体上相似的集合聚到一起，缩小比对的范围，这样只用检测较少的集合对，就可以找到绝大多数相似的集合对，大幅度减少时间开销。虽然牺牲了一部分精度，但是如果能够将时间大幅度减少，这种算法还是可以接受的。接下来的内容讲解如何使用Minhash和LSH（Locality-sensitive Hashing）来实现上述目的，在相似的集合较少的情况下，可以在O(n)时间找到大部分相似的集合对。

minhash用于降维，lsh 局部敏感hash 用于划分相似集合，其基本思路是将相似的集合聚集到一起，减小查找范围，避免比较不相似的集合 
Minhash算法大体思路是：采用一种hash函数，将元素的位置均匀打乱，然后将新顺序下每个集合第一个值为1的元素作为该集合的特征值。
